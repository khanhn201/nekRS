#!/bin/bash
set -e

#--------------------------------------
: ${QUEUE:="lustre_scaling"}
: ${NEKRS_GPU_MPI:=0}
: ${NEKRS_BACKEND:="dpcpp"}
: ${RANKS_FOR_BUILD:=12}
: ${SIM_RANKS_PER_NODE:=6}
: ${TRAIN_RANKS_PER_NODE:=6}
: ${DEPLOYMENT:="colocated"}
: ${SIM_NODES:=1}
: ${TRAIN_NODES:=1}
: ${SIM_CPU_BIND_LIST:="1:8:16:24:32:40"}
: ${TRAIN_CPU_BIND_LIST:="53:60:68:76:84:92"}
: ${INFERENCE_CPU_BIND_LIST:="1:8:16:24:32:40:53:60:68:76:84:92"}
: ${OCCA_DPCPP_COMPILER_FLAGS:="-O3 -fsycl -fsycl-targets=intel_gpu_pvc -ftarget-register-alloc-mode=pvc:auto -fma"}
: ${ONEAPI_SDK:=""}
: ${FRAMEWORKS_MODULE:="frameworks"}
: ${VENV_PATH:=""}
#--------------------------------------

cp turbChannel.par.safe turbChannel.par

source $NEKRS_HOME/bin/nrsqsub_utils
setup $# 1

chk_case $TOTAL_RANKS

#--------------------------------------
# Generate the affinity scripts
SAFF_FILE=affinity_nrs.sh
TAFF_FILE=affinity_ml.sh
if [ ${ZE_FLAT_DEVICE_HIERARCHY} == "COMPOSITE" ] || [[ -z "${ZE_FLAT_DEVICE_HIERARCHY}" ]]; then
  echo "#!/bin/bash" > $SAFF_FILE
  echo "num_gpus=\$1" >> $SAFF_FILE
  echo "shift" >> $SAFF_FILE
  echo "num_tiles=2" >> $SAFF_FILE
  echo "gpu_id=\$(((PALS_LOCAL_RANKID / \${num_tiles}) % \${num_gpus}))" >> $SAFF_FILE
  echo "tile_id=\$((PALS_LOCAL_RANKID % \${num_tiles}))" >> $SAFF_FILE
  echo "export ZE_AFFINITY_MASK=\$gpu_id.\$tile_id" >> $SAFF_FILE
  echo "exec \"\$@\"" >> $SAFF_FILE

  echo "#!/bin/bash" > $TAFF_FILE
  echo "num_gpus=\$1" >> $TAFF_FILE
  echo "offset=\$2" >> $TAFF_FILE
  echo "shift" >> $TAFF_FILE
  echo "shift" >> $TAFF_FILE
  echo "num_tiles=2" >> $TAFF_FILE
  echo "gpu_id=\$(((PALS_LOCAL_RANKID / \${num_tiles}) % \${num_gpus} + (offset / num_tiles) ))" >> $TAFF_FILE
  echo "tile_id=\$((PALS_LOCAL_RANKID % \${num_tiles}))" >> $TAFF_FILE
  echo "export ZE_AFFINITY_MASK=\$gpu_id.\$tile_id" >> $TAFF_FILE
  echo "exec \"\$@\"" >> $TAFF_FILE
elif [ ${ZE_FLAT_DEVICE_HIERARCHY} == "FLAT" ]; then
  echo "#!/bin/bash" > $SAFF_FILE
  echo "num_gpus=\$1" >> $SAFF_FILE
  echo "shift" >> $SAFF_FILE
  echo "gpu_id=\$((PALS_LOCAL_RANKID % \${num_gpus} ))" >> $SAFF_FILE
  echo "export ZE_AFFINITY_MASK=\$gpu_id" >> $SAFF_FILE
  echo "exec \"\$@\"" >> $SAFF_FILE

  echo "#!/bin/bash" > $TAFF_FILE
  echo "num_gpus=\$1" >> $TAFF_FILE
  echo "offset=\$2" >> $TAFF_FILE
  echo "shift" >> $TAFF_FILE
  echo "shift" >> $TAFF_FILE
  echo "gpu_id=\$((PALS_LOCAL_RANKID % \${num_gpus} + \${offset} ))" >> $TAFF_FILE
  echo "export ZE_AFFINITY_MASK=\$gpu_id" >> $TAFF_FILE
  echo "exec \"\$@\"" >> $TAFF_FILE
fi
chmod u+x $SAFF_FILE $TAFF_FILE

#--------------------------------------
# Generate the workflow config script
CFILE=config.yaml
echo "# Workflow config" > $CFILE
echo "scheduler: pbs" >> $CFILE
echo "deployment: \"${DEPLOYMENT}\"" >> $CFILE
echo "" >> $CFILE
echo "# Run config" >> $CFILE
echo "run_args:" >> $CFILE
echo "    nodes: ${qnodes}" >> $CFILE
if [ ${DEPLOYMENT} == "colocated"  ]; then
  SIM_NODES=$nodes
  TRAIN_NODES=$nodes
  SIM_PROCS=$(( SIM_NODES * SIM_RANKS_PER_NODE ))
  TRAIN_PROCS=$(( TRAIN_NODES * TRAIN_RANKS_PER_NODE ))
  echo "    sim_nodes: ${SIM_NODES}" >> $CFILE
  echo "    ml_nodes: ${SIM_NODES}" >> $CFILE
  echo "    simprocs: ${SIM_PROCS}" >> $CFILE
  echo "    simprocs_pn: ${SIM_RANKS_PER_NODE}" >> $CFILE
  echo "    mlprocs: ${TRAIN_PROCS}" >> $CFILE
  echo "    mlprocs_pn: ${TRAIN_RANKS_PER_NODE}" >> $CFILE
  echo "    sim_cpu_bind: \"list:${SIM_CPU_BIND_LIST}\"" >> $CFILE
  echo "    ml_cpu_bind: \"list:${TRAIN_CPU_BIND_LIST}\"" >> $CFILE
elif [ ${DEPLOYMENT} == "clustered"  ]; then
  SIM_PROCS=$(( SIM_NODES * SIM_RANKS_PER_NODE ))
  TRAIN_PROCS=$(( TRAIN_NODES * TRAIN_RANKS_PER_NODE ))
  echo "    sim_nodes: ${SIM_NODES}" >> $CFILE
  echo "    ml_nodes: ${TRAIN_NODES}" >> $CFILE
  echo "    simprocs: ${SIM_PROCS}" >> $CFILE
  echo "    simprocs_pn: ${SIM_RANKS_PER_NODE}" >> $CFILE
  echo "    mlprocs: ${TRAIN_PROCS}" >> $CFILE
  echo "    mlprocs_pn: ${TRAIN_RANKS_PER_NODE}" >> $CFILE
  echo "    sim_cpu_bind: \"list:${SIM_CPU_BIND_LIST}\"" >> $CFILE
  echo "    ml_cpu_bind: \"list:${SIM_CPU_BIND_LIST}\"" >> $CFILE
fi
echo "" >> $CFILE
echo "# Simulation config" >> $CFILE
echo "sim:" >> $CFILE
echo "    executable: \"${NEKRS_HOME}/bin/nekrs\"" >> $CFILE
echo "    arguments: \"--setup ${case}.par --backend ${NEKRS_BACKEND} --device-id 0\"" >> $CFILE
echo "    affinity: \"./${SAFF_FILE}\"" >> $CFILE
echo "" >> $CFILE
echo "# Trainer config" >> $CFILE
echo "train:" >> $CFILE
echo "    executable: \"${NEKRS_HOME}/3rd_party/gnn/main.py\"" >> $CFILE
echo "    affinity: \"./${TAFF_FILE}\"" >> $CFILE
echo "    arguments: \"backend=ccl halo_swap_mode=none online=True client.backend=adios consistency=False time_dependency=time_dependent verbose=True\"" >> $CFILE
echo "" >> $CFILE
echo "# Inference config" >> $CFILE
echo "inference:" >> $CFILE
echo "    executable: \"${NEKRS_HOME}/3rd_party/gnn/inference.py\"" >> $CFILE
echo "    affinity: \"./${TAFF_FILE}\"" >> $CFILE
echo "    arguments: \"model_task=inference rollout_steps=100 backend=ccl halo_swap_mode=none online=True client.backend=adios consistency=False time_dependency=time_dependent verbose=True\"" >> $CFILE

#--------------------------------------
# Update the .par file with the ML options
PFILE=${case}.par
echo "" >> $PFILE
echo "[ML]" >> $PFILE
echo "adiosEngine = SST" >> $PFILE
echo "adiosTransport = WAN" >> $PFILE
echo "adiosStream = sync" >> $PFILE

#--------------------------------------
# Update the .box file to increase the mesh size linearly with the number of nodes
NX=$(( 36*SIM_NODES ))
NZ=$(( 18*SIM_NODES ))
cp ${case}_1node.box ${case}.box
sed -i "s/NX/${NX}/g" ${case}.box
sed -i "s/NZ/${NZ}/g" ${case}.box

#--------------------------------------
# Generate the run script
RFILE=run.sh
echo "#!/bin/bash" > $RFILE

echo "export TZ='/usr/share/zoneinfo/US/Central'" >> $RFILE

echo -e "\necho Jobid: \$PBS_JOBID" >>$RFILE
echo "echo Running on host \`hostname\`" >>$RFILE
echo "echo Running on nodes \`cat \$PBS_NODEFILE\`" >>$RFILE

echo "module load ${FRAMEWORKS_MODULE}" >> $RFILE
echo "source ${VENV_PATH}" >> $RFILE
echo "module list" >> $RFILE

echo -e "\nexport NEKRS_HOME=$NEKRS_HOME" >>$RFILE
echo "export NEKRS_GPU_MPI=$NEKRS_GPU_MPI" >>$RFILE
#echo "export MPICH_GPU_SUPPORT_ENABLED=$NEKRS_GPU_MPI" >> $RFILE

echo "export OCCA_DPCPP_COMPILER_FLAGS=\"$OCCA_DPCPP_COMPILER_FLAGS\"" >> $RFILE

# Workaround for MPICH 52.2 see https://docs.alcf.anl.gov/aurora/known-issues/
#echo "unset MPIR_CVAR_CH4_COLL_SELECTION_TUNING_JSON_FILE" >> $RFILE
#echo "unset MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE" >> $RFILE
#echo "unset MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE" >> $RFILE

# Cray MPI libfabric defaults
echo "export FI_CXI_RDZV_THRESHOLD=16384" >> $RFILE
echo "export FI_CXI_RDZV_EAGER_SIZE=2048" >> $RFILE
echo "export FI_CXI_DEFAULT_CQ_SIZE=131072" >> $RFILE
echo "export FI_CXI_DEFAULT_TX_SIZE=1024" >> $RFILE
echo "export FI_CXI_OFLOW_BUF_SIZE=12582912" >> $RFILE
echo "export FI_CXI_OFLOW_BUF_COUNT=3" >> $RFILE
echo "export FI_CXI_REQ_BUF_MIN_POSTED=6" >> $RFILE
echo "export FI_CXI_REQ_BUF_SIZE=12582912" >> $RFILE
echo "export FI_MR_CACHE_MAX_SIZE=-1" >> $RFILE
echo "export FI_MR_CACHE_MAX_COUNT=524288" >> $RFILE
echo "export FI_CXI_REQ_BUF_MAX_CACHED=0" >> $RFILE
echo "export FI_CXI_REQ_BUF_MIN_POSTED=6" >> $RFILE
# echo "export FI_CXI_RX_MATCH_MODE=hardware" >> $RFILE
echo "export FI_CXI_RX_MATCH_MODE=hybrid" >> $RFILE # required by parRSB

echo -e "\nexport PYTHONPATH=\$PYTHONPATH:${NEKRS_HOME}/lib/python3.10/site-packages" >> $RFILE
echo "#export SstVerbose=1" >> $RFILE
echo "export OMP_PROC_BIND=spread" >> $RFILE
echo "export OMP_PLACES=threads" >> $RFILE
echo "if ls *.sst 1> /dev/null 2>&1" >> $RFILE
echo "then" >> $RFILE 
echo "    echo Cleaning up old .sst files" >> $RFILE
echo "    rm *.sst" >> $RFILE
echo "fi" >> $RFILE

echo -e "\n# precompilation" >>$RFILE
CMD_build="mpiexec -n ${RANKS_FOR_BUILD} -ppn ${RANKS_FOR_BUILD} --cpu-bind list:1:8:16:24:32:40:53:60:68:76:84:92 -- ./${SAFF_FILE} ${RANKS_FOR_BUILD} $bin --setup ${case} --backend ${NEKRS_BACKEND} --device-id 0 $extra_args --build-only ${RANKS_FOR_BUILD}"
add_build_CMD "$RFILE" "$CMD_build" "$TOTAL_RANKS"

echo -e "\n# actual run" >>$RFILE
echo "python driver.py" >> $RFILE
chmod u+x $RFILE

